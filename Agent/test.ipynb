{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56f25921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import engine\n",
    "from model import Base\n",
    "Base.metadata.create_all(bind=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7e15a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base.metadata.drop_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25547d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone_text.sparse import BM25Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35e3cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from tools.retriver_tool import create_retriever_tool\n",
    "from document_processor import process_new_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea518786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1823e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()    \n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0a9d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_retriever = create_retriever_tool(\n",
    "    index_name=\"db\",\n",
    "    tool_name=\"document_retriever\",\n",
    "    description=\"Search knowledge base for document content relevant to a query\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d8e8a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a professional document analyst assistant. Use tools to find relevant information.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "\n",
    "# retrieval_chain = create_stuff_documents_chain(llm,prompt)\n",
    "agent = create_tool_calling_agent(llm, [document_retriever], prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=[document_retriever],\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e56f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_scanner import scan_directory\n",
    "def process_and_query(query: str, directory: str = None) -> str:\n",
    "    if directory:\n",
    "        scan_directory(directory)\n",
    "        print(\"Documents queued for background processing...\")\n",
    "    \n",
    "    result = agent_executor.invoke({\"input\": query})\n",
    "    return result['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed90e88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path:  /home/thkr1408/Projects/Inquiro/uploads/attention.pdf\n",
      "Skipping processed file: attention.pdf\n",
      "File path:  /home/thkr1408/Projects/Inquiro/uploads/speech.txt\n",
      "Skipping processed file: speech.txt\n",
      "File path:  /home/thkr1408/Projects/Inquiro/uploads/pdfs\n",
      "File path:  /home/thkr1408/Projects/Inquiro/uploads/images\n",
      "File path:  /home/thkr1408/Projects/Inquiro/uploads/docs\n",
      "Documents queued for background processing...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `document_retriever` with `Attention Is All You Need authors`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/attention.pdf, Page: 13]\n",
      "\n",
      "the approach we take in our model.\n",
      "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\n",
      "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/attention.pdf, Page: 6]\n",
      "\n",
      "the matrix of outputs as:\n",
      "Attention(Q, K, V) = softmax(QKT\n",
      "√dk\n",
      ")V (1)\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of 1√dk\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/attention.pdf, Page: 3]\n",
      "\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/attention.pdf, Page: 0]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `document_retriever` with `\"Attention Is All You Need\" authors`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe Transformer uses multi-head attention in three different ways:\n",
      "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/attention.pdf, Page: 4]\n",
      "\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/attention.pdf, Page: 3]\n",
      "\n",
      "of his statement so that saying what he truly felt this would be the best option.[7] Nonetheless, he did add the qualifier \"if needs be\".[10]\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/speech.txt, Page: None]\n",
      "\n",
      "Mandela worked on the speech for weeks before the trial, receiving help in editing and polishing it from author Nadine Gordimer and journalist Anthony Sampson.[8] In writing the speech, Mandela was inspired by Fidel Castro's \"History Will Absolve Me\" defence speech. He was particularly interested\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/speech.txt, Page: None]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `document_retriever` with `\"Attention Is All You Need\" \"Vaswani\"`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe Transformer uses multi-head attention in three different ways:\n",
      "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/attention.pdf, Page: 4]\n",
      "\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/attention.pdf, Page: 3]\n",
      "\n",
      "of his statement so that saying what he truly felt this would be the best option.[7] Nonetheless, he did add the qualifier \"if needs be\".[10]\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/speech.txt, Page: None]\n",
      "\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/attention.pdf, Page: 13]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `document_retriever` with `\"Attention Is All You Need\" 2017 authors`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe Transformer uses multi-head attention in three different ways:\n",
      "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/attention.pdf, Page: 4]\n",
      "\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/attention.pdf, Page: 3]\n",
      "\n",
      "ray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\n",
      "2017.\n",
      "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "In International Conference on Learning Representations, 2017.\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/attention.pdf, Page: 10]\n",
      "\n",
      "Mandela worked on the speech for weeks before the trial, receiving help in editing and polishing it from author Nadine Gordimer and journalist Anthony Sampson.[8] In writing the speech, Mandela was inspired by Fidel Castro's \"History Will Absolve Me\" defence speech. He was particularly interested\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/speech.txt, Page: None]\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `document_retriever` with `\"Attention Is All You Need\" \"Ashish Vaswani\"`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mProvided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/attention.pdf, Page: 0]\n",
      "\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/attention.pdf, Page: 4]\n",
      "\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/attention.pdf, Page: 0]\n",
      "\n",
      "of his statement so that saying what he truly felt this would be the best option.[7] Nonetheless, he did add the qualifier \"if needs be\".[10]\n",
      "[Source: /home/thkr1408/Projects/Inquiro/uploads/speech.txt, Page: None]\u001b[0m\u001b[32;1m\u001b[1;3m**“Attention Is All You Need”** – the seminal Transformer paper published in 2017 – was authored by the following researchers:\n",
      "\n",
      "| # | Author |\n",
      "|---|--------|\n",
      "| 1 | **Ashish Vaswani** (Google Brain) |\n",
      "| 2 | **Noam Shazeer** (Google Brain) |\n",
      "| 3 | **Niki Parmar** (Google Research) |\n",
      "| 4 | **Jakob Uszkoreit** (Google Research) |\n",
      "| 5 | **Llion Jones** (Google Research) |\n",
      "| 6 | **Aidan N. Gomez** (University of Toronto) |\n",
      "| 7 | **Łukasz Kaiser** (Google Brain) |\n",
      "| 8 | **Illia Polosukhin** (Independent researcher) |\n",
      "\n",
      "These eight authors are listed on the title page of the paper and are credited with developing the Transformer architecture that introduced the “attention‑is‑all‑you‑need” paradigm.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "**“Attention Is All You Need”** – the seminal Transformer paper published in 2017 – was authored by the following researchers:\n",
      "\n",
      "| # | Author |\n",
      "|---|--------|\n",
      "| 1 | **Ashish Vaswani** (Google Brain) |\n",
      "| 2 | **Noam Shazeer** (Google Brain) |\n",
      "| 3 | **Niki Parmar** (Google Research) |\n",
      "| 4 | **Jakob Uszkoreit** (Google Research) |\n",
      "| 5 | **Llion Jones** (Google Research) |\n",
      "| 6 | **Aidan N. Gomez** (University of Toronto) |\n",
      "| 7 | **Łukasz Kaiser** (Google Brain) |\n",
      "| 8 | **Illia Polosukhin** (Independent researcher) |\n",
      "\n",
      "These eight authors are listed on the title page of the paper and are credited with developing the Transformer architecture that introduced the “attention‑is‑all‑you‑need” paradigm.\n"
     ]
    }
   ],
   "source": [
    "response = process_and_query(\"Authors of attention is all you need?\", \"/home/thkr1408/Projects/Inquiro/uploads/\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20879c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Inquiro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
